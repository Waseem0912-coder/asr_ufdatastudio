{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/ctc_seg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from ctc_segmentation import ctc_segmentation, prepare_text, CtcSegmentationParameters\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/blue/ufdatastudios/ahmed.waseem/ctc/meta_speaker.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['duration'].apply(lambda x: x >= 5)]\n",
    "audio_folder = \"/blue/ufdatastudios/ahmed.waseem/processed_audio\"\n",
    "df[\"audio_filepath\"] = df[\"audio_filepath\"].apply(lambda x: os.path.join(audio_folder, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_filepath</th>\n",
       "      <th>duration</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>age-group</th>\n",
       "      <th>primary_language</th>\n",
       "      <th>native_place_state</th>\n",
       "      <th>native_place_district</th>\n",
       "      <th>highest_qualification</th>\n",
       "      <th>job_category</th>\n",
       "      <th>occupation_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>and they went off into the space when earth wa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>45-60</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>Sambalpur</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Education and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I should say addictive because once I started ...</td>\n",
       "      <td>Female</td>\n",
       "      <td>45-60</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>Sambalpur</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Education and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>and with the kind of dishes that you have in I...</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-30</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Ratnagiri</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Part Time</td>\n",
       "      <td>Social service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>So in order to bring all these back, in order ...</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-30</td>\n",
       "      <td>Malayalam</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Pathanamthitta</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Education and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3856</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It's a well-known fact that India is a country...</td>\n",
       "      <td>Female</td>\n",
       "      <td>30-45</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>Goa</td>\n",
       "      <td>South Goa</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Technology and Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         audio_filepath  duration  \\\n",
       "3852  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3853  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3854  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3855  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3856  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "\n",
       "                                                   text  gender age-group  \\\n",
       "3852  and they went off into the space when earth wa...  Female     45-60   \n",
       "3853  I should say addictive because once I started ...  Female     45-60   \n",
       "3854  and with the kind of dishes that you have in I...  Female     18-30   \n",
       "3855  So in order to bring all these back, in order ...  Female     18-30   \n",
       "3856  It's a well-known fact that India is a country...  Female     30-45   \n",
       "\n",
       "     primary_language native_place_state native_place_district  \\\n",
       "3852           Telugu             Odisha             Sambalpur   \n",
       "3853           Telugu             Odisha             Sambalpur   \n",
       "3854          Marathi        Maharashtra             Ratnagiri   \n",
       "3855        Malayalam             Kerala        Pathanamthitta   \n",
       "3856            Hindi                Goa             South Goa   \n",
       "\n",
       "     highest_qualification job_category        occupation_domain  \n",
       "3852              Graduate    Full Time   Education and Research  \n",
       "3853              Graduate    Full Time   Education and Research  \n",
       "3854              Graduate    Part Time           Social service  \n",
       "3855         Post Graduate    Full Time   Education and Research  \n",
       "3856         Post Graduate    Full Time  Technology and Services  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['audio_filepath', 'duration', 'text', 'gender', 'age-group',\n",
       "       'primary_language', 'native_place_state', 'native_place_district',\n",
       "       'highest_qualification', 'job_category', 'occupation_domain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filepaths = df['audio_filepath'].tolist()\n",
    "transcripts = df['text'].tolist()\n",
    "text = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize CTC segmentation parameters with skip_prob adjustment\n",
    "config = CtcSegmentationParameters()\n",
    "config.blank = processor.tokenizer.pad_token_id\n",
    "config.skip_prob = 1.0  # Allows skipping parts of the text if audio is shorter\n",
    "config.max_prob = 0.95  # Adjust as needed based on segmentation tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get logits from the audio file\n",
    "def get_logits(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    \n",
    "    # Ensure waveform is 2D with shape [1, sequence_length]\n",
    "    if waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)  # Add a channel dimension if missing\n",
    "    \n",
    "    # Pass waveform as a list to maintain the batch dimension (1, sequence_length)\n",
    "    inputs = processor(waveform.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get logits from model\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.to(model.device)).logits.cpu()\n",
    "    \n",
    "    # Confirm logits are 2D with shape [time_steps, vocab_size]\n",
    "    if logits.ndim == 3:\n",
    "        logits = logits.squeeze(0)  # Remove the batch dimension if it exists and only if it’s a 3D tensor\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Buffer has wrong number of dimensions (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run CTC segmentation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m timing, char_list \u001b[38;5;241m=\u001b[39m \u001b[43mctc_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlpz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Aggregate subword alignments to approximate word-level alignments\u001b[39;00m\n\u001b[1;32m     20\u001b[0m word_alignments \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/ctc_seg/lib/python3.10/site-packages/ctc_segmentation/ctc_segmentation.py:159\u001b[0m, in \u001b[0;36mctc_segmentation\u001b[0;34m(config, lpz, ground_truth)\u001b[0m\n\u001b[1;32m    157\u001b[0m offsets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mlen\u001b[39m(ground_truth)], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Run actual alignment of utterances\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m t, c \u001b[38;5;241m=\u001b[39m \u001b[43mcython_fill_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlpz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbacktrack_from_max_t:\n\u001b[1;32m    168\u001b[0m     t \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32mctc_segmentation/ctc_segmentation_dyn.pyx:20\u001b[0m, in \u001b[0;36mctc_segmentation.ctc_segmentation_dyn.cython_fill_table\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop over DataFrame to process each audio file and align with CTC segmentation\n",
    "aligned_data = []\n",
    "for _, row in df.iterrows():\n",
    "    audio_path = row[\"audio_filepath\"]\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate logits\n",
    "    logits = get_logits(audio_path)\n",
    "    \n",
    "    # Convert logits to log probabilities (required for CTC segmentation)\n",
    "    lpz = torch.nn.functional.log_softmax(logits, dim=-1).numpy()\n",
    "    \n",
    "    # Tokenize the text and prepare ground truth\n",
    "    ground_truth = processor.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids[0].tolist()\n",
    "    \n",
    "    # Run CTC segmentation\n",
    "    timing, char_list = ctc_segmentation(config, lpz, ground_truth)\n",
    "    \n",
    "    # Aggregate subword alignments to approximate word-level alignments\n",
    "    word_alignments = []\n",
    "    current_word = \"\"\n",
    "    word_start_time = None\n",
    "    \n",
    "    for idx, (start, end) in enumerate(zip(timing[\"begin\"], timing[\"end\"])):\n",
    "        subword = processor.tokenizer.decode([ground_truth[idx]])\n",
    "        \n",
    "        # Start a new word if the previous one ended\n",
    "        if subword.strip() != \"\":\n",
    "            if current_word == \"\":\n",
    "                word_start_time = start\n",
    "            \n",
    "            current_word += subword\n",
    "            \n",
    "            # If the subword completes a word (based on space in text)\n",
    "            if \" \" in text[len(\" \".join([w[\"word\"] for w in word_alignments])) :].strip().split(\" \", 1)[0]:\n",
    "                word_alignments.append({\n",
    "                    \"word\": current_word.strip(),\n",
    "                    \"start_time\": word_start_time,\n",
    "                    \"end_time\": end\n",
    "                })\n",
    "                current_word = \"\"\n",
    "                word_start_time = None\n",
    "    \n",
    "    # Append to aligned_data with filename reference\n",
    "    aligned_data.append({\"audio_filepath\": audio_path, \"text\": text, \"word_alignments\": word_alignments})\n",
    "\n",
    "# Save the aligned data in a new DataFrame\n",
    "aligned_df = pd.DataFrame(aligned_data)\n",
    "\n",
    "# Display the aligned DataFrame\n",
    "aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '<', '>', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor vocabulary size: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Processor vocabulary size:\", len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m ctc_segmentation(params, logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), prepared_text)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Print alignment details\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end, word \u001b[38;5;129;01min\u001b[39;00m segmentation:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, End: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTC segmentation completed for file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 222198\n",
      "Logits length: torch.Size([1, 249, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\"Text length:\", len(prepared_text))\n",
    "print(\"Logits length:\", logits_list[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  2.6421, -17.2083, -16.9143,  ...,  -3.9510,  -4.9777,  -3.8921],\n",
       "         [  8.2383, -24.9574, -24.4365,  ...,  -7.0936,  -7.7027,  -3.9959],\n",
       "         [  9.4998, -26.5420, -26.2310,  ...,  -7.4192,  -6.5949,  -5.1694],\n",
       "         ...,\n",
       "         [  6.4110, -21.9786, -21.2738,  ...,  -5.9054,  -3.6497,  -5.7050],\n",
       "         [  7.6090, -22.5000, -21.7588,  ...,  -7.2212,  -4.0831,  -6.0867],\n",
       "         [  6.8192, -22.1714, -21.4689,  ...,  -6.6844,  -3.8365,  -5.9234]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctc",
   "language": "python=3.10",
   "name": "ctc_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
