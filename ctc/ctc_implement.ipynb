{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from ctc_segmentation import ctc_segmentation, prepare_text, CtcSegmentationParameters\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/blue/ufdatastudios/ahmed.waseem/ctc/meta_speaker.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['duration'].apply(lambda x: x >= 5)]\n",
    "audio_folder = \"/blue/ufdatastudios/ahmed.waseem/processed_audio\"\n",
    "df[\"audio_filepath\"] = df[\"audio_filepath\"].apply(lambda x: os.path.join(audio_folder, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_filepath</th>\n",
       "      <th>duration</th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "      <th>age-group</th>\n",
       "      <th>primary_language</th>\n",
       "      <th>native_place_state</th>\n",
       "      <th>native_place_district</th>\n",
       "      <th>highest_qualification</th>\n",
       "      <th>job_category</th>\n",
       "      <th>occupation_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3852</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>and they went off into the space when earth wa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>45-60</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>Sambalpur</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Education and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I should say addictive because once I started ...</td>\n",
       "      <td>Female</td>\n",
       "      <td>45-60</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>Sambalpur</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Education and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>and with the kind of dishes that you have in I...</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-30</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Ratnagiri</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Part Time</td>\n",
       "      <td>Social service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3855</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>So in order to bring all these back, in order ...</td>\n",
       "      <td>Female</td>\n",
       "      <td>18-30</td>\n",
       "      <td>Malayalam</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>Pathanamthitta</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Education and Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3856</th>\n",
       "      <td>/blue/ufdatastudios/ahmed.waseem/processed_aud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It's a well-known fact that India is a country...</td>\n",
       "      <td>Female</td>\n",
       "      <td>30-45</td>\n",
       "      <td>Hindi</td>\n",
       "      <td>Goa</td>\n",
       "      <td>South Goa</td>\n",
       "      <td>Post Graduate</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Technology and Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         audio_filepath  duration  \\\n",
       "3852  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3853  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3854  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3855  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "3856  /blue/ufdatastudios/ahmed.waseem/processed_aud...       5.0   \n",
       "\n",
       "                                                   text  gender age-group  \\\n",
       "3852  and they went off into the space when earth wa...  Female     45-60   \n",
       "3853  I should say addictive because once I started ...  Female     45-60   \n",
       "3854  and with the kind of dishes that you have in I...  Female     18-30   \n",
       "3855  So in order to bring all these back, in order ...  Female     18-30   \n",
       "3856  It's a well-known fact that India is a country...  Female     30-45   \n",
       "\n",
       "     primary_language native_place_state native_place_district  \\\n",
       "3852           Telugu             Odisha             Sambalpur   \n",
       "3853           Telugu             Odisha             Sambalpur   \n",
       "3854          Marathi        Maharashtra             Ratnagiri   \n",
       "3855        Malayalam             Kerala        Pathanamthitta   \n",
       "3856            Hindi                Goa             South Goa   \n",
       "\n",
       "     highest_qualification job_category        occupation_domain  \n",
       "3852              Graduate    Full Time   Education and Research  \n",
       "3853              Graduate    Full Time   Education and Research  \n",
       "3854              Graduate    Part Time           Social service  \n",
       "3855         Post Graduate    Full Time   Education and Research  \n",
       "3856         Post Graduate    Full Time  Technology and Services  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['audio_filepath', 'duration', 'text', 'gender', 'age-group',\n",
       "       'primary_language', 'native_place_state', 'native_place_district',\n",
       "       'highest_qualification', 'job_category', 'occupation_domain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filepaths = df['audio_filepath'].tolist()\n",
    "transcripts = df['text'].tolist()\n",
    "text = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "model_name = \"facebook/wav2vec2-large-960h\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCTC.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize CTC segmentation parameters\n",
    "config = CtcSegmentationParameters()\n",
    "config.blank = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get logits from the audio file\n",
    "def get_logits(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Ensure the waveform is resampled if needed\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    \n",
    "    # Remove any unnecessary dimensions and process the audio\n",
    "    waveform = waveform.squeeze()  # Make sure waveform is 1D before feeding it to the processor\n",
    "    \n",
    "    # Pass waveform as a list to keep the batch dimension (1, sequence_length)\n",
    "    inputs = processor(waveform.tolist(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get logits from model\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.to(model.device)).logits.cpu()\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Audio is shorter than text!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run CTC segmentation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m timing, char_list \u001b[38;5;241m=\u001b[39m \u001b[43mctc_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlpz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Aggregate subword alignments to approximate word-level alignments\u001b[39;00m\n\u001b[1;32m     20\u001b[0m word_alignments \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/blue/arthur.porto-biocosmos/ahmed.waseem/.conda/envs/ctc_seg/lib/python3.10/site-packages/ctc_segmentation/ctc_segmentation.py:147\u001b[0m, in \u001b[0;36mctc_segmentation\u001b[0;34m(config, lpz, ground_truth)\u001b[0m\n\u001b[1;32m    141\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTC segmentation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ground_truth)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chars \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_duration\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms audio \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlpz\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m indices).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ground_truth) \u001b[38;5;241m>\u001b[39m lpz\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mskip_prob \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_prob:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio is shorter than text!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m window_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmin_window_size\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Try multiple window lengths if it fails\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Audio is shorter than text!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop over DataFrame to process each audio file and align with CTC segmentation\n",
    "aligned_data = []\n",
    "for _, row in df.iterrows():\n",
    "    audio_path = row[\"audio_filepath\"]\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Generate logits\n",
    "    logits = get_logits(audio_path)\n",
    "    \n",
    "    # Convert logits to log probabilities (required for CTC segmentation)\n",
    "    lpz = torch.nn.functional.log_softmax(logits, dim=-1).numpy()\n",
    "    \n",
    "    # Tokenize the text and prepare ground truth\n",
    "    ground_truth = processor.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids[0].tolist()\n",
    "    \n",
    "    # Run CTC segmentation\n",
    "    timing, char_list = ctc_segmentation(config, lpz, ground_truth)\n",
    "    \n",
    "    # Aggregate subword alignments to approximate word-level alignments\n",
    "    word_alignments = []\n",
    "    current_word = \"\"\n",
    "    word_start_time = None\n",
    "    \n",
    "    for idx, (start, end) in enumerate(zip(timing[\"begin\"], timing[\"end\"])):\n",
    "        subword = processor.tokenizer.decode([ground_truth[idx]])\n",
    "        \n",
    "        # Start a new word if the previous one ended\n",
    "        if subword.strip() != \"\":\n",
    "            if current_word == \"\":\n",
    "                word_start_time = start\n",
    "            \n",
    "            current_word += subword\n",
    "            \n",
    "            # If the subword completes a word (based on space in text)\n",
    "            if \" \" in text[len(\" \".join([w[\"word\"] for w in word_alignments])) :].strip().split(\" \", 1)[0]:\n",
    "                word_alignments.append({\n",
    "                    \"word\": current_word.strip(),\n",
    "                    \"start_time\": word_start_time,\n",
    "                    \"end_time\": end\n",
    "                })\n",
    "                current_word = \"\"\n",
    "                word_start_time = None\n",
    "    \n",
    "    # Append to aligned_data with filename reference\n",
    "    aligned_data.append({\"audio_filepath\": audio_path, \"text\": text, \"word_alignments\": word_alignments})\n",
    "\n",
    "# Save the aligned data in a new DataFrame\n",
    "aligned_df = pd.DataFrame(aligned_data)\n",
    "\n",
    "# Display the aligned DataFrame\n",
    "aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '<', '>', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Assuming your MFA dictionary is in a file called `mfa_dict.txt`\n",
    "mfa_dict_path = \"/home/ahmed.waseem/Documents/MFA/pretrained_models/dictionary/english_india_mfa.dict\"\n",
    "\n",
    "unique_chars = set()\n",
    "\n",
    "# Read the MFA dictionary file\n",
    "with open(mfa_dict_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Split each line by whitespace to extract the word (first part)\n",
    "        if line.strip():  # Make sure the line is not empty\n",
    "            word = line.split()[0]  # The word is the first element before the phonemes\n",
    "            # Add characters of the word to the set\n",
    "            unique_chars.update(list(word))\n",
    "\n",
    "# Convert the set to a sorted list to create a consistent char_list\n",
    "char_list = sorted(unique_chars)\n",
    "\n",
    "# Print the character list for verification\n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor vocabulary size: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"Processor vocabulary size:\", len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m ctc_segmentation(params, logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), prepared_text)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Print alignment details\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end, word \u001b[38;5;129;01min\u001b[39;00m segmentation:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Start: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, End: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTC segmentation completed for file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from ctc_segmentation import ctc_segmentation, CtcSegmentationParameters, prepare_text, determine_utterance_segments\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model and processor\n",
    "model_name = \"facebook/wav2vec2-large-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Directory to save resampled audio files\n",
    "resampled_audio_folder = 'processed_audio'\n",
    "os.makedirs(resampled_audio_folder, exist_ok=True)\n",
    "\n",
    "# Function to resample and save audio files\n",
    "def resample_and_save_audio(audio_path, target_sample_rate=16000):\n",
    "    resampled_audio_path = os.path.join(resampled_audio_folder, os.path.basename(audio_path))\n",
    "    if os.path.exists(resampled_audio_path):\n",
    "        return resampled_audio_path\n",
    "    \n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    torchaudio.save(resampled_audio_path, waveform, target_sample_rate)\n",
    "    return resampled_audio_path\n",
    "\n",
    "# CTC segmentation setup\n",
    "params = CtcSegmentationParameters()\n",
    "char_list = processor.tokenizer.convert_ids_to_tokens(range(processor.tokenizer.vocab_size))\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    audio_path = row['audio_filepath']\n",
    "    transcript = row['text']\n",
    "\n",
    "    # Resample and save the audio file if necessary\n",
    "    resampled_audio_path = resample_and_save_audio(audio_path)\n",
    "\n",
    "    # Load the resampled audio\n",
    "    waveform, _ = torchaudio.load(resampled_audio_path)\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform[0]  # Take the first channel if stereo\n",
    "\n",
    "    # Prepare input features for the Wav2Vec2 model\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    # Extract logits from the model without computing gradients\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Prepare text for CTC segmentation\n",
    "    prepared_text, char_list = prepare_text(params, [transcript], char_list)\n",
    "\n",
    "    # Perform CTC segmentation\n",
    "    params.char_list = char_list  # Update character list in segmentation parameters\n",
    "    segmentation = ctc_segmentation(params, logits.squeeze(0).cpu().numpy(), prepared_text)\n",
    "\n",
    "    # Print alignment details\n",
    "    for start, end, word in segmentation:\n",
    "        print(f\"Word: '{word}', Start: {start:.2f}s, End: {end:.2f}s\")\n",
    "\n",
    "    print(f\"CTC segmentation completed for file {index + 1}/{len(df)}: {audio_path}\")\n",
    "\n",
    "print(\"CTC segmentation completed for all audio files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 222198\n",
      "Logits length: torch.Size([1, 249, 32])\n"
     ]
    }
   ],
   "source": [
    "print(\"Text length:\", len(prepared_text))\n",
    "print(\"Logits length:\", logits_list[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  2.6421, -17.2083, -16.9143,  ...,  -3.9510,  -4.9777,  -3.8921],\n",
       "         [  8.2383, -24.9574, -24.4365,  ...,  -7.0936,  -7.7027,  -3.9959],\n",
       "         [  9.4998, -26.5420, -26.2310,  ...,  -7.4192,  -6.5949,  -5.1694],\n",
       "         ...,\n",
       "         [  6.4110, -21.9786, -21.2738,  ...,  -5.9054,  -3.6497,  -5.7050],\n",
       "         [  7.6090, -22.5000, -21.7588,  ...,  -7.2212,  -4.0831,  -6.0867],\n",
       "         [  6.8192, -22.1714, -21.4689,  ...,  -6.6844,  -3.8365,  -5.9234]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctc",
   "language": "python=3.10",
   "name": "ctc_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
